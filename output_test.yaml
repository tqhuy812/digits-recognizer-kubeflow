apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: digits-recognizer-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.1, pipelines.kubeflow.org/pipeline_compilation_time: '2023-09-04T23:19:38.837744',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Detect digits", "inputs":
      [{"name": "no_epochs"}, {"name": "optimizer"}], "name": "digits-recognizer-pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.1}
spec:
  entrypoint: digits-recognizer-pipeline
  templates:
  - name: digits-recognizer-pipeline
    inputs:
      parameters:
      - {name: no_epochs}
      - {name: optimizer}
    dag:
      tasks:
      - {name: get-data-batch, template: get-data-batch}
      - {name: get-latest-data, template: get-latest-data}
      - name: model-building
        template: model-building
        dependencies: [reshape-data]
        arguments:
          parameters:
          - {name: no_epochs, value: '{{inputs.parameters.no_epochs}}'}
          - {name: optimizer, value: '{{inputs.parameters.optimizer}}'}
      - name: model-serving
        template: model-serving
        dependencies: [model-building]
      - name: reshape-data
        template: reshape-data
        dependencies: [get-data-batch, get-latest-data]
  - name: get-data-batch
    container:
      args: ['----output-paths', /tmp/outputs/datapoints_training/data, /tmp/outputs/datapoints_test/data,
        /tmp/outputs/dataset_version/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def get_data_batch():
            """
            Function to get dataset and load it to minio bucket
            """
            print("getting data")
            from tensorflow import keras
            from minio import Minio
            import numpy as np
            import json

            minio_client = Minio(
                # "172.17.0.35:9000",
                "minio-service.kubeflow.svc.cluster.local:9000",
                access_key="minio",
                secret_key="minio123",
                secure=False
            )
            minio_bucket = "mlpipeline"

            minio_client.fget_object(minio_bucket,"mnist.npz","/tmp/mnist.npz")

            def load_data():
                with np.load("/tmp/mnist.npz", allow_pickle=True) as f:
                    x_train, y_train = f["x_train"], f["y_train"]
                    x_test, y_test = f["x_test"], f["y_test"]

                return (x_train, y_train), (x_test, y_test)

            # Get MNIST data directly from library
            (x_train, y_train), (x_test, y_test) = load_data()

            # save to numpy file, store in Minio
            np.save("/tmp/x_train.npy",x_train)
            minio_client.fput_object(minio_bucket,"x_train","/tmp/x_train.npy")

            np.save("/tmp/y_train.npy",y_train)
            minio_client.fput_object(minio_bucket,"y_train","/tmp/y_train.npy")

            np.save("/tmp/x_test.npy",x_test)
            minio_client.fput_object(minio_bucket,"x_test","/tmp/x_test.npy")

            np.save("/tmp/y_test.npy",y_test)
            minio_client.fput_object(minio_bucket,"y_test","/tmp/y_test.npy")

            dataset_version = "1.0"

            print(f"x_train shape: {x_train.shape}")
            print(f"y_train shape: {y_train.shape}")

            print(f"x_test shape: {x_test.shape}")
            print(f"y_test shape: {y_test.shape}")

            from collections import namedtuple
            divmod_output = namedtuple('Outputs', ['datapoints_training', 'datapoints_test', 'dataset_version'])
            return [float(x_train.shape[0]),float(x_test.shape[0]),dataset_version]

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(
                    str(float_value), str(type(float_value))))
            return str(float_value)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Get data batch', description='Function to get dataset and load it to minio bucket')
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=3)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = get_data_batch(**_parsed_args)

        _output_serializers = [
            _serialize_float,
            _serialize_float,
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
    outputs:
      artifacts:
      - {name: get-data-batch-datapoints_test, path: /tmp/outputs/datapoints_test/data}
      - {name: get-data-batch-datapoints_training, path: /tmp/outputs/datapoints_training/data}
      - {name: get-data-batch-dataset_version, path: /tmp/outputs/dataset_version/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.1
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Function
          to get dataset and load it to minio bucket", "implementation": {"container":
          {"args": ["----output-paths", {"outputPath": "datapoints_training"}, {"outputPath":
          "datapoints_test"}, {"outputPath": "dataset_version"}], "command": ["sh",
          "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def get_data_batch():\n    \"\"\"\n    Function
          to get dataset and load it to minio bucket\n    \"\"\"\n    print(\"getting
          data\")\n    from tensorflow import keras\n    from minio import Minio\n    import
          numpy as np\n    import json\n\n    minio_client = Minio(\n        # \"172.17.0.35:9000\",\n        \"minio-service.kubeflow.svc.cluster.local:9000\",\n        access_key=\"minio\",\n        secret_key=\"minio123\",\n        secure=False\n    )\n    minio_bucket
          = \"mlpipeline\"\n\n    minio_client.fget_object(minio_bucket,\"mnist.npz\",\"/tmp/mnist.npz\")\n\n    def
          load_data():\n        with np.load(\"/tmp/mnist.npz\", allow_pickle=True)
          as f:\n            x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n            x_test,
          y_test = f[\"x_test\"], f[\"y_test\"]\n\n        return (x_train, y_train),
          (x_test, y_test)\n\n    # Get MNIST data directly from library\n    (x_train,
          y_train), (x_test, y_test) = load_data()\n\n    # save to numpy file, store
          in Minio\n    np.save(\"/tmp/x_train.npy\",x_train)\n    minio_client.fput_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n\n    np.save(\"/tmp/y_train.npy\",y_train)\n    minio_client.fput_object(minio_bucket,\"y_train\",\"/tmp/y_train.npy\")\n\n    np.save(\"/tmp/x_test.npy\",x_test)\n    minio_client.fput_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n\n    np.save(\"/tmp/y_test.npy\",y_test)\n    minio_client.fput_object(minio_bucket,\"y_test\",\"/tmp/y_test.npy\")\n\n    dataset_version
          = \"1.0\"\n\n    print(f\"x_train shape: {x_train.shape}\")\n    print(f\"y_train
          shape: {y_train.shape}\")\n\n    print(f\"x_test shape: {x_test.shape}\")\n    print(f\"y_test
          shape: {y_test.shape}\")\n\n    from collections import namedtuple\n    divmod_output
          = namedtuple(''Outputs'', [''datapoints_training'', ''datapoints_test'',
          ''dataset_version''])\n    return [float(x_train.shape[0]),float(x_test.shape[0]),dataset_version]\n\ndef
          _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,
          str):\n        return float_value\n    if not isinstance(float_value, (float,
          int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          float.''.format(\n            str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\ndef _serialize_str(str_value: str) -> str:\n    if not
          isinstance(str_value, str):\n        raise TypeError(''Value \"{}\" has
          type \"{}\" instead of str.''.format(\n            str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Get
          data batch'', description=''Function to get dataset and load it to minio
          bucket'')\n_parser.add_argument(\"----output-paths\", dest=\"_output_paths\",
          type=str, nargs=3)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = get_data_batch(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_float,\n    _serialize_float,\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0"}},
          "name": "Get data batch", "outputs": [{"name": "datapoints_training", "type":
          "Float"}, {"name": "datapoints_test", "type": "Float"}, {"name": "dataset_version",
          "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: get-latest-data
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def get_latest_data():
            """
            Dummy functions for showcasing
            """
            print("Adding latest data")

        import argparse
        _parser = argparse.ArgumentParser(prog='Get latest data', description='Dummy functions for showcasing')
        _parsed_args = vars(_parser.parse_args())

        _outputs = get_latest_data(**_parsed_args)
      image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.1
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Dummy
          functions for showcasing", "implementation": {"container": {"args": [],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def get_latest_data():\n    \"\"\"\n    Dummy
          functions for showcasing\n    \"\"\"\n    print(\"Adding latest data\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Get latest data'', description=''Dummy
          functions for showcasing'')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = get_latest_data(**_parsed_args)\n"], "image": "public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0"}},
          "name": "Get latest data"}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: model-building
    container:
      args: [--no-epochs, '{{inputs.parameters.no_epochs}}', --optimizer, '{{inputs.parameters.optimizer}}',
        '----output-paths', /tmp/outputs/mlpipeline_ui_metadata/data, /tmp/outputs/mlpipeline_metrics/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def model_building(
            no_epochs = 1,
            optimizer = "adam"
        ):
            """
            Build the model with Keras API
            Export model parameters
            """
            from tensorflow import keras
            import tensorflow as tf
            from minio import Minio
            import numpy as np
            import pandas as pd
            import json

            minio_client = Minio(
                # "172.17.0.35:9000",
                "minio-service.kubeflow.svc.cluster.local:9000",
                access_key="minio",
                secret_key="minio123",
                secure=False
            )
            minio_bucket = "mlpipeline"

            model = keras.models.Sequential()
            model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)))
            model.add(keras.layers.MaxPool2D(2, 2))

            model.add(keras.layers.Flatten())
            model.add(keras.layers.Dense(64, activation='relu'))

            model.add(keras.layers.Dense(32, activation='relu'))

            model.add(keras.layers.Dense(10, activation='softmax')) #output are 10 classes, numbers from 0-9

            #show model summary - how it looks
            stringlist = []
            model.summary(print_fn=lambda x: stringlist.append(x))
            metric_model_summary = "\n".join(stringlist)

            #compile the model - we want to have a binary outcome
            model.compile(optimizer=optimizer,
                      loss="sparse_categorical_crossentropy",
                      metrics=['accuracy'])

            minio_client.fget_object(minio_bucket,"x_train","/tmp/x_train.npy")
            x_train = np.load("/tmp/x_train.npy")

            minio_client.fget_object(minio_bucket,"y_train","/tmp/y_train.npy")
            y_train = np.load("/tmp/y_train.npy")

            #fit the model and return the history while training
            history = model.fit(
              x=x_train,
              y=y_train,
              epochs=no_epochs,
              batch_size=20,
            )

            minio_client.fget_object(minio_bucket,"x_test","/tmp/x_test.npy")
            x_test = np.load("/tmp/x_test.npy")

            minio_client.fget_object(minio_bucket,"y_test","/tmp/y_test.npy")
            y_test = np.load("/tmp/y_test.npy")

            # Test the model against the test dataset
            # Returns the loss value & metrics values for the model in test mode.
            model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)

            # Confusion Matrix

            # Generates output predictions for the input samples.
            test_predictions = model.predict(x=x_test)

            # Returns the indices of the maximum values along an axis.
            test_predictions = np.argmax(test_predictions,axis=1) # the prediction outputs 10 values, we take the index number of the highest value, which is the prediction of the model

            # generate confusion matrix
            confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)
            confusion_matrix = confusion_matrix.numpy()
            vocab = list(np.unique(y_test))
            data = []
            for target_index, target_row in enumerate(confusion_matrix):
                for predicted_index, count in enumerate(target_row):
                    data.append((vocab[target_index], vocab[predicted_index], count))

            df_cm = pd.DataFrame(data, columns=['target', 'predicted', 'count'])
            cm_csv = df_cm.to_csv(header=False, index=False)

            metadata = {
                "outputs": [
                    {
                        "type": "confusion_matrix",
                        "format": "csv",
                        "schema": [
                            {'name': 'target', 'type': 'CATEGORY'},
                            {'name': 'predicted', 'type': 'CATEGORY'},
                            {'name': 'count', 'type': 'NUMBER'},
                          ],
                        "target_col" : "actual",
                        "predicted_col" : "predicted",
                        "source": cm_csv,
                        "storage": "inline",
                        "labels": [0,1,2,3,4,5,6,7,8,9]
                    },
                    {
                        'storage': 'inline',
                        'source': '''# Model Overview
        ## Model Summary

        ```
        {}
        ```

        ## Model Performance

        **Accuracy**: {}
        **Loss**: {}

        '''.format(metric_model_summary,model_accuracy,model_loss),
                        'type': 'markdown',
                    }
                ]
            }

            metrics = {
              'metrics': [{
                  'name': 'model_accuracy',
                  'numberValue':  float(model_accuracy),
                  'format' : "PERCENTAGE"
                },{
                  'name': 'model_loss',
                  'numberValue':  float(model_loss),
                  'format' : "PERCENTAGE"
                }]}

            ### Save model to minIO

            keras.models.save_model(model,"/tmp/detect-digits")

            from minio import Minio
            import os

            minio_client = Minio(
                    # "172.17.0.35:9000",
                    "minio-service.kubeflow.svc.cluster.local:9000",
                    access_key="minio",
                    secret_key="minio123",
                    secure=False
                )
            minio_bucket = "mlpipeline"

            import glob

            def upload_local_directory_to_minio(local_path, bucket_name, minio_path):
                assert os.path.isdir(local_path)

                for local_file in glob.glob(local_path + '/**'):
                    local_file = local_file.replace(os.sep, "/") # Replace \ with / on Windows
                    if not os.path.isfile(local_file):
                        upload_local_directory_to_minio(
                            local_file, bucket_name, minio_path + "/" + os.path.basename(local_file))
                    else:
                        remote_path = os.path.join(
                            minio_path, local_file[1 + len(local_path):])
                        remote_path = remote_path.replace(
                            os.sep, "/")  # Replace \ with / on Windows
                        minio_client.fput_object(bucket_name, remote_path, local_file)

            upload_local_directory_to_minio("/tmp/detect-digits",minio_bucket,"models/detect-digits/1/") # 1 for version 1

            print("Saved model to minIO")

            from collections import namedtuple
            output = namedtuple('output', ['mlpipeline_ui_metadata', 'mlpipeline_metrics'])
            return output(json.dumps(metadata),json.dumps(metrics))

        import argparse
        _parser = argparse.ArgumentParser(prog='Model building', description='Build the model with Keras API')
        _parser.add_argument("--no-epochs", dest="no_epochs", type=int, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("--optimizer", dest="optimizer", type=str, required=False, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=2)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = model_building(**_parsed_args)

        _output_serializers = [
            str,
            str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
    inputs:
      parameters:
      - {name: no_epochs}
      - {name: optimizer}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /tmp/outputs/mlpipeline_ui_metadata/data}
      - {name: mlpipeline-metrics, path: /tmp/outputs/mlpipeline_metrics/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.1
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Build
          the model with Keras API", "implementation": {"container": {"args": [{"if":
          {"cond": {"isPresent": "no_epochs"}, "then": ["--no-epochs", {"inputValue":
          "no_epochs"}]}}, {"if": {"cond": {"isPresent": "optimizer"}, "then": ["--optimizer",
          {"inputValue": "optimizer"}]}}, "----output-paths", {"outputPath": "mlpipeline_ui_metadata"},
          {"outputPath": "mlpipeline_metrics"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def model_building(\n    no_epochs = 1,\n    optimizer = \"adam\"\n):\n    \"\"\"\n    Build
          the model with Keras API\n    Export model parameters\n    \"\"\"\n    from
          tensorflow import keras\n    import tensorflow as tf\n    from minio import
          Minio\n    import numpy as np\n    import pandas as pd\n    import json\n\n    minio_client
          = Minio(\n        # \"172.17.0.35:9000\",\n        \"minio-service.kubeflow.svc.cluster.local:9000\",\n        access_key=\"minio\",\n        secret_key=\"minio123\",\n        secure=False\n    )\n    minio_bucket
          = \"mlpipeline\"\n\n    model = keras.models.Sequential()\n    model.add(keras.layers.Conv2D(64,
          (3, 3), activation=''relu'', input_shape=(28,28,1)))\n    model.add(keras.layers.MaxPool2D(2,
          2))\n\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(64,
          activation=''relu''))\n\n    model.add(keras.layers.Dense(32, activation=''relu''))\n\n    model.add(keras.layers.Dense(10,
          activation=''softmax'')) #output are 10 classes, numbers from 0-9\n\n    #show
          model summary - how it looks\n    stringlist = []\n    model.summary(print_fn=lambda
          x: stringlist.append(x))\n    metric_model_summary = \"\\n\".join(stringlist)\n\n    #compile
          the model - we want to have a binary outcome\n    model.compile(optimizer=optimizer,\n              loss=\"sparse_categorical_crossentropy\",\n              metrics=[''accuracy''])\n\n    minio_client.fget_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n    x_train
          = np.load(\"/tmp/x_train.npy\")\n\n    minio_client.fget_object(minio_bucket,\"y_train\",\"/tmp/y_train.npy\")\n    y_train
          = np.load(\"/tmp/y_train.npy\")\n\n    #fit the model and return the history
          while training\n    history = model.fit(\n      x=x_train,\n      y=y_train,\n      epochs=no_epochs,\n      batch_size=20,\n    )\n\n    minio_client.fget_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n    x_test
          = np.load(\"/tmp/x_test.npy\")\n\n    minio_client.fget_object(minio_bucket,\"y_test\",\"/tmp/y_test.npy\")\n    y_test
          = np.load(\"/tmp/y_test.npy\")\n\n    # Test the model against the test
          dataset\n    # Returns the loss value & metrics values for the model in
          test mode.\n    model_loss, model_accuracy = model.evaluate(x=x_test,y=y_test)\n\n    #
          Confusion Matrix\n\n    # Generates output predictions for the input samples.\n    test_predictions
          = model.predict(x=x_test)\n\n    # Returns the indices of the maximum values
          along an axis.\n    test_predictions = np.argmax(test_predictions,axis=1)
          # the prediction outputs 10 values, we take the index number of the highest
          value, which is the prediction of the model\n\n    # generate confusion
          matrix\n    confusion_matrix = tf.math.confusion_matrix(labels=y_test,predictions=test_predictions)\n    confusion_matrix
          = confusion_matrix.numpy()\n    vocab = list(np.unique(y_test))\n    data
          = []\n    for target_index, target_row in enumerate(confusion_matrix):\n        for
          predicted_index, count in enumerate(target_row):\n            data.append((vocab[target_index],
          vocab[predicted_index], count))\n\n    df_cm = pd.DataFrame(data, columns=[''target'',
          ''predicted'', ''count''])\n    cm_csv = df_cm.to_csv(header=False, index=False)\n\n    metadata
          = {\n        \"outputs\": [\n            {\n                \"type\": \"confusion_matrix\",\n                \"format\":
          \"csv\",\n                \"schema\": [\n                    {''name'':
          ''target'', ''type'': ''CATEGORY''},\n                    {''name'': ''predicted'',
          ''type'': ''CATEGORY''},\n                    {''name'': ''count'', ''type'':
          ''NUMBER''},\n                  ],\n                \"target_col\" : \"actual\",\n                \"predicted_col\"
          : \"predicted\",\n                \"source\": cm_csv,\n                \"storage\":
          \"inline\",\n                \"labels\": [0,1,2,3,4,5,6,7,8,9]\n            },\n            {\n                ''storage'':
          ''inline'',\n                ''source'': ''''''# Model Overview\n## Model
          Summary\n\n```\n{}\n```\n\n## Model Performance\n\n**Accuracy**: {}\n**Loss**:
          {}\n\n''''''.format(metric_model_summary,model_accuracy,model_loss),\n                ''type'':
          ''markdown'',\n            }\n        ]\n    }\n\n    metrics = {\n      ''metrics'':
          [{\n          ''name'': ''model_accuracy'',\n          ''numberValue'':  float(model_accuracy),\n          ''format''
          : \"PERCENTAGE\"\n        },{\n          ''name'': ''model_loss'',\n          ''numberValue'':  float(model_loss),\n          ''format''
          : \"PERCENTAGE\"\n        }]}\n\n    ### Save model to minIO\n\n    keras.models.save_model(model,\"/tmp/detect-digits\")\n\n    from
          minio import Minio\n    import os\n\n    minio_client = Minio(\n            #
          \"172.17.0.35:9000\",\n            \"minio-service.kubeflow.svc.cluster.local:9000\",\n            access_key=\"minio\",\n            secret_key=\"minio123\",\n            secure=False\n        )\n    minio_bucket
          = \"mlpipeline\"\n\n    import glob\n\n    def upload_local_directory_to_minio(local_path,
          bucket_name, minio_path):\n        assert os.path.isdir(local_path)\n\n        for
          local_file in glob.glob(local_path + ''/**''):\n            local_file =
          local_file.replace(os.sep, \"/\") # Replace \\ with / on Windows\n            if
          not os.path.isfile(local_file):\n                upload_local_directory_to_minio(\n                    local_file,
          bucket_name, minio_path + \"/\" + os.path.basename(local_file))\n            else:\n                remote_path
          = os.path.join(\n                    minio_path, local_file[1 + len(local_path):])\n                remote_path
          = remote_path.replace(\n                    os.sep, \"/\")  # Replace \\
          with / on Windows\n                minio_client.fput_object(bucket_name,
          remote_path, local_file)\n\n    upload_local_directory_to_minio(\"/tmp/detect-digits\",minio_bucket,\"models/detect-digits/1/\")
          # 1 for version 1\n\n    print(\"Saved model to minIO\")\n\n    from collections
          import namedtuple\n    output = namedtuple(''output'', [''mlpipeline_ui_metadata'',
          ''mlpipeline_metrics''])\n    return output(json.dumps(metadata),json.dumps(metrics))\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Model building'', description=''Build
          the model with Keras API'')\n_parser.add_argument(\"--no-epochs\", dest=\"no_epochs\",
          type=int, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--optimizer\",
          dest=\"optimizer\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = model_building(**_parsed_args)\n\n_output_serializers
          = [\n    str,\n    str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0"}},
          "inputs": [{"default": "1", "name": "no_epochs", "optional": true, "type":
          "Integer"}, {"default": "adam", "name": "optimizer", "optional": true, "type":
          "String"}], "name": "Model building", "outputs": [{"name": "mlpipeline_ui_metadata",
          "type": "UI_metadata"}, {"name": "mlpipeline_metrics", "type": "Metrics"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"no_epochs":
          "{{inputs.parameters.no_epochs}}", "optimizer": "{{inputs.parameters.optimizer}}"}'}
  - name: model-serving
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'kserve==0.8.0.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'kserve==0.8.0.1' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def model_serving():\n    \"\"\"\n    Create kserve instance\n    \"\"\"\n\
        \    from kubernetes import client \n    from kserve import KServeClient\n\
        \    from kserve import constants\n    from kserve import utils\n    from\
        \ kserve import V1beta1InferenceService\n    from kserve import V1beta1InferenceServiceSpec\n\
        \    from kserve import V1beta1PredictorSpec\n    from kserve import V1beta1TFServingSpec\n\
        \    from datetime import datetime\n\n    namespace = utils.get_default_target_namespace()\n\
        \n    now = datetime.now()\n    v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n\
        \n    name='digits-recognizer-{}'.format(v)\n    kserve_version='v1beta1'\n\
        \    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n\n    isvc\
        \ = V1beta1InferenceService(api_version=api_version,\n                   \
        \                kind=constants.KSERVE_KIND,\n                           \
        \        metadata=client.V1ObjectMeta(\n                                 \
        \      name=name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n\
        \                                   spec=V1beta1InferenceServiceSpec(\n  \
        \                                 predictor=V1beta1PredictorSpec(\n      \
        \                                 service_account_name=\"sa-minio-kserve\"\
        ,\n                                       tensorflow=(V1beta1TFServingSpec(\n\
        \                                           storage_uri=\"s3://mlpipeline/models/detect-digits/\"\
        ))))\n    )\n\n    KServe = KServeClient()\n    KServe.create(isvc)\n\nimport\
        \ argparse\n_parser = argparse.ArgumentParser(prog='Model serving', description='Create\
        \ kserve instance')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs\
        \ = model_serving(**_parsed_args)\n"
      image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.1
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Create
          kserve instance", "implementation": {"container": {"args": [], "command":
          ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''kserve==0.8.0.1'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''kserve==0.8.0.1''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def model_serving():\n    \"\"\"\n    Create kserve instance\n    \"\"\"\n    from
          kubernetes import client \n    from kserve import KServeClient\n    from
          kserve import constants\n    from kserve import utils\n    from kserve import
          V1beta1InferenceService\n    from kserve import V1beta1InferenceServiceSpec\n    from
          kserve import V1beta1PredictorSpec\n    from kserve import V1beta1TFServingSpec\n    from
          datetime import datetime\n\n    namespace = utils.get_default_target_namespace()\n\n    now
          = datetime.now()\n    v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n\n    name=''digits-recognizer-{}''.format(v)\n    kserve_version=''v1beta1''\n    api_version
          = constants.KSERVE_GROUP + ''/'' + kserve_version\n\n    isvc = V1beta1InferenceService(api_version=api_version,\n                                   kind=constants.KSERVE_KIND,\n                                   metadata=client.V1ObjectMeta(\n                                       name=name,
          namespace=namespace, annotations={''sidecar.istio.io/inject'':''false''}),\n                                   spec=V1beta1InferenceServiceSpec(\n                                   predictor=V1beta1PredictorSpec(\n                                       service_account_name=\"sa-minio-kserve\",\n                                       tensorflow=(V1beta1TFServingSpec(\n                                           storage_uri=\"s3://mlpipeline/models/detect-digits/\"))))\n    )\n\n    KServe
          = KServeClient()\n    KServe.create(isvc)\n\nimport argparse\n_parser =
          argparse.ArgumentParser(prog=''Model serving'', description=''Create kserve
          instance'')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs = model_serving(**_parsed_args)\n"],
          "image": "public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0"}},
          "name": "Model serving"}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: reshape-data
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def reshape_data():
            """
            Reshape the data for model building
            """
            print("reshaping data")

            from minio import Minio
            import numpy as np

            minio_client = Minio(
                # "172.17.0.35:9000",
                "minio-service.kubeflow.svc.cluster.local:9000",
                access_key="minio",
                secret_key="minio123",
                secure=False
            )
            minio_bucket = "mlpipeline"

            # load data from minio
            minio_client.fget_object(minio_bucket,"x_train","/tmp/x_train.npy")
            x_train = np.load("/tmp/x_train.npy")

            minio_client.fget_object(minio_bucket,"x_test","/tmp/x_test.npy")
            x_test = np.load("/tmp/x_test.npy")

            # reshaping the data
            # reshaping pixels in a 28x28px image with greyscale, canal = 1. This is needed for the Keras API
            x_train = x_train.reshape(-1,28,28,1)
            x_test = x_test.reshape(-1,28,28,1)

            # normalizing the data
            # each pixel has a value between 0-255. Here we divide by 255, to get values from 0-1
            x_train = x_train / 255
            x_test = x_test / 255

            # save data from minio
            np.save("/tmp/x_train.npy",x_train)
            minio_client.fput_object(minio_bucket,"x_train","/tmp/x_train.npy")

            np.save("/tmp/x_test.npy",x_test)
            minio_client.fput_object(minio_bucket,"x_test","/tmp/x_test.npy")

        import argparse
        _parser = argparse.ArgumentParser(prog='Reshape data', description='Reshape the data for model building')
        _parsed_args = vars(_parser.parse_args())

        _outputs = reshape_data(**_parsed_args)
      image: public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.1
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Reshape
          the data for model building", "implementation": {"container": {"args": [],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def reshape_data():\n    \"\"\"\n    Reshape
          the data for model building\n    \"\"\"\n    print(\"reshaping data\")\n\n    from
          minio import Minio\n    import numpy as np\n\n    minio_client = Minio(\n        #
          \"172.17.0.35:9000\",\n        \"minio-service.kubeflow.svc.cluster.local:9000\",\n        access_key=\"minio\",\n        secret_key=\"minio123\",\n        secure=False\n    )\n    minio_bucket
          = \"mlpipeline\"\n\n    # load data from minio\n    minio_client.fget_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n    x_train
          = np.load(\"/tmp/x_train.npy\")\n\n    minio_client.fget_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n    x_test
          = np.load(\"/tmp/x_test.npy\")\n\n    # reshaping the data\n    # reshaping
          pixels in a 28x28px image with greyscale, canal = 1. This is needed for
          the Keras API\n    x_train = x_train.reshape(-1,28,28,1)\n    x_test = x_test.reshape(-1,28,28,1)\n\n    #
          normalizing the data\n    # each pixel has a value between 0-255. Here we
          divide by 255, to get values from 0-1\n    x_train = x_train / 255\n    x_test
          = x_test / 255\n\n    # save data from minio\n    np.save(\"/tmp/x_train.npy\",x_train)\n    minio_client.fput_object(minio_bucket,\"x_train\",\"/tmp/x_train.npy\")\n\n    np.save(\"/tmp/x_test.npy\",x_test)\n    minio_client.fput_object(minio_bucket,\"x_test\",\"/tmp/x_test.npy\")\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Reshape data'', description=''Reshape
          the data for model building'')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = reshape_data(**_parsed_args)\n"], "image": "public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow-full:v1.5.0"}},
          "name": "Reshape data"}', pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters:
    - {name: no_epochs}
    - {name: optimizer}
  serviceAccountName: pipeline-runner
